{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Display entire Scenario string in notebook\n",
    "pd.options.display.max_colwidth = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_DSS(DSS_file_path, output_tab):\n",
    "    \n",
    "    # Read data from Excel\n",
    "    columns_from_excel = 'F,KC,KD,KE,KU,LE,MK'\n",
    "    column_headers = ['scenarios', 'cash_delta', 'finance_delta', 'lease_delta', 'spend_delta', 'lift_delta','elasticity']\n",
    "    df = pd.read_excel(DSS_file_path, sheet_name='Calc', names=column_headers, skiprows=499, nrows=500, usecols=columns_from_excel)\n",
    "    \n",
    "    # Read chosen scenarios from output_tab and append to df\n",
    "    chosen_scenarios = pd.read_excel(DSS_file_path, names=['target_scenarios'], sheet_name=output_tab, usecols='C', skiprows=35, nrows=46)\n",
    "    chosen_scenarios = chosen_scenarios.dropna()\n",
    "    chosen_scenarios = chosen_scenarios['target_scenarios'].astype('int')\n",
    "    df['target_scenarios'] = 0\n",
    "    for index, row in chosen_scenarios.iteritems():\n",
    "        df['target_scenarios'].iloc[row-1] = 1\n",
    "        \n",
    "    # Remove (#) and spaces at beginning and end of Scenario\n",
    "    df['scenarios'] = df['scenarios'].str.replace('\\\\(.\\\\)','', regex=True).str.lstrip().str.rstrip()\n",
    "\n",
    "    # Create delta_spend columns in data\n",
    "    # Note: need to add user input baseline (currently controlled in if index % 500 == x)\n",
    "    delta_columns = ['cash_delta', 'finance_delta', 'lease_delta','spend_delta','lift_delta']\n",
    "    for x in delta_columns:\n",
    "        baseline = 0\n",
    "        df_delta = []\n",
    "        if x == 'lift_delta':\n",
    "            for index, row in df.iterrows():\n",
    "                if index % 500 == 11:\n",
    "                    baseline = row[x]\n",
    "                try:\n",
    "                    delta = row[x]/baseline - 1\n",
    "                    df_delta.append(delta)\n",
    "                except:\n",
    "                    delta = row[x] - baseline\n",
    "                    df_delta.append(delta)\n",
    "        else:\n",
    "            for index, row in df.iterrows():\n",
    "                if index % 500 == 11:\n",
    "                    baseline = row[x]\n",
    "                delta = row[x] - baseline\n",
    "                df_delta.append(delta)\n",
    "        df[x] = df_delta\n",
    "\n",
    "    # Create no_of_moves column\n",
    "    no_of_moves = 0\n",
    "    df_no_of_moves = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        no_of_moves = str(row['scenarios']).count('\\n') + 1\n",
    "        df_no_of_moves.append(no_of_moves)\n",
    "\n",
    "    df['no_of_moves'] = df_no_of_moves\n",
    "\n",
    "    # Find efficient frontier (only model #1 so far)\n",
    "    df_length = df.shape[0]\n",
    "    eff_front = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,df_length,df_length):\n",
    "        for k in range(13,df_length):\n",
    "            current_spend = df['spend_delta'][k + i]\n",
    "            current_lift = df['lift_delta'][k + i]\n",
    "            for j in range(13,df_length):\n",
    "                new_spend = df['spend_delta'][j + i]\n",
    "                new_lift = df['lift_delta'][j + i]\n",
    "                if (new_spend < current_spend) & (new_lift > current_lift):\n",
    "                    break\n",
    "                elif (np.isnan(df['spend_delta'][j + i])) & (j == df_length-1):\n",
    "                    if np.isnan(df['spend_delta'][k + i]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        eff_front = eff_front.append(df.iloc[[k+i]])\n",
    "\n",
    "    # Drop N/As\n",
    "    eff_front = eff_front.dropna()\n",
    "\n",
    "    return eff_front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "DSS_wkbks = [r'C:\\Users\\bryant.vu\\Documents\\Python_Scripts\\2019.11.04 - DSS - v4.7.2 - 7 block - Mitsu MY19 v4.xlsm',r'C:\\Users\\bryant.vu\\Documents\\Python_Scripts\\2019.12.11 - DSS - v4.7.2 - 7 block - Mitsu MY19 v4.xlsm',r'C:\\Users\\bryant.vu\\Documents\\Python_Scripts\\2019.12.11 - DSS - v4.7.2 - 7 block - Mitsu MY20 v5.xlsm']\n",
    "\n",
    "scenarios = pd.DataFrame()\n",
    "for i in DSS_wkbks:\n",
    "    df = read_from_DSS(i, '(1)')\n",
    "    scenarios = scenarios.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove single lever moves (add user option in later)\n",
    "scenarios['lift/spend'] = scenarios['lift_delta'] / scenarios['spend_delta']\n",
    "scenarios = scenarios[scenarios['no_of_moves'] > 1].sort_values('spend_delta')\n",
    "\n",
    "# Reduce APR moves with std\n",
    "reg_ex = 'std'\n",
    "reg_ex_filter = scenarios['scenarios'].str.contains(reg_ex)\n",
    "scenarios = scenarios[~reg_ex_filter]\n",
    "\n",
    "# Remove +CC and -APR moves\n",
    "reg_ex = 'APR'\n",
    "reg_ex_filter = scenarios['scenarios'].str.contains(reg_ex)\n",
    "scenarios['cash_finance_sum'] = round(scenarios['cash_delta']/50.0)*50 + round(scenarios['finance_delta']/50.0)*50\n",
    "cash_finance_sum_filter = scenarios['cash_finance_sum'] == 0\n",
    "\n",
    "scenarios = scenarios[~(reg_ex_filter & cash_finance_sum_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scenarios.drop(['target_scenarios', 'scenarios', 'cash_delta', 'finance_delta', 'lease_delta', 'lift/spend', 'cash_finance_sum'], axis=1), scenarios['target_scenarios'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train LogReg model\n",
    "LogReg = LogisticRegression()\n",
    "LogReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9393939393939394"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score model\n",
    "LogReg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=True, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train RF model\n",
    "model = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9375"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
